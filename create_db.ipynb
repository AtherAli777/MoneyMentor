{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ather Ali\\anaconda3\\envs\\fyp\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text.\"\"\"\n",
    "    # Remove special characters but retain letters, digits, and whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove extra whitespace and blank lines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_document(file_path, chunk_size=1000, chunk_overlap=20):\n",
    "    if file_path.lower().endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    else:\n",
    "        loader = TextLoader(file_path=file_path, encoding='utf-8')\n",
    "    \n",
    "    try:\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} document(s) from {file_path}\")\n",
    "        \n",
    "        # Clean the text of each document\n",
    "        cleaned_documents = [clean_text(doc.page_content) for doc in documents]\n",
    "        \n",
    "        # Join all cleaned documents into a single string\n",
    "        full_text = \" \".join(cleaned_documents)\n",
    "        print(f\"Text cleaned. Total length: {len(full_text)} characters\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        # Create Document objects\n",
    "        processed_documents = [Document(page_content=chunk, metadata={\"source\": file_path}) for chunk in chunks]\n",
    "        \n",
    "        print(f\"Processed into {len(processed_documents)} document chunks\")\n",
    "        return processed_documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pgvector_db(db_name, collection_name, texts):\n",
    "    connection_string = f\"postgresql+psycopg2://postgres:1234@localhost:5432/{db_name}\"\n",
    "    \n",
    "    db = PGVector.from_documents(\n",
    "        embedding=embeddings,\n",
    "        documents=texts,\n",
    "        collection_name=collection_name,\n",
    "        connection_string=connection_string\n",
    "    )\n",
    "    print(f\"Successfully created {db_name}\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    r'rich_poor_dad.pdf',\n",
    "    r'total-money.pdf',\n",
    "    r'your-money.pdf',\n",
    "    r'millionaire_fastlane.pdf',\n",
    "    r'the-psychology-money.pdf',\n",
    "    r'the-intelligent-investor.pdf',\n",
    "    r'fast-and-slow.pdf',\n",
    "    r'principles-life.pdf',\n",
    "    r'teach-you-to-be-rich.pdf',\n",
    "    r'the-little-book.pdf',\n",
    "    r'the-millionaire.pdf',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 232 document(s) from rich_poor_dad.pdf\n",
      "Text cleaned. Total length: 339797 characters\n",
      "Processed into 347 document chunks\n",
      "Loaded 251 document(s) from total-money.pdf\n",
      "Text cleaned. Total length: 420906 characters\n",
      "Processed into 430 document chunks\n",
      "Loaded 448 document(s) from your-money.pdf\n",
      "Text cleaned. Total length: 658806 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed into 673 document chunks\n",
      "Loaded 341 document(s) from millionaire_fastlane.pdf\n",
      "Text cleaned. Total length: 723674 characters\n",
      "Processed into 739 document chunks\n",
      "Loaded 214 document(s) from the-psychology-money.pdf\n",
      "Text cleaned. Total length: 298103 characters\n",
      "Processed into 305 document chunks\n",
      "Loaded 629 document(s) from the-intelligent-investor.pdf\n",
      "Text cleaned. Total length: 1144209 characters\n",
      "Processed into 1168 document chunks\n",
      "Loaded 683 document(s) from fast-and-slow.pdf\n",
      "Text cleaned. Total length: 1132383 characters\n",
      "Processed into 1156 document chunks\n",
      "Loaded 538 document(s) from principles-life.pdf\n",
      "Text cleaned. Total length: 900788 characters\n",
      "Processed into 920 document chunks\n",
      "Loaded 508 document(s) from teach-you-to-be-rich.pdf\n",
      "Text cleaned. Total length: 638397 characters\n",
      "Processed into 652 document chunks\n",
      "Loaded 185 document(s) from the-little-book.pdf\n",
      "Text cleaned. Total length: 264804 characters\n",
      "Processed into 271 document chunks\n",
      "Loaded 327 document(s) from the-millionaire.pdf\n",
      "Text cleaned. Total length: 537550 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed into 549 document chunks\n",
      "Loaded 341 document(s) from millionaire_fastlane.pdf\n",
      "Text cleaned. Total length: 723674 characters\n",
      "Processed into 739 document chunks\n"
     ]
    }
   ],
   "source": [
    "processed_texts = [load_and_process_document(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created rich_poor_dad\n",
      "Successfully created total-money\n",
      "Successfully created your-money\n",
      "Successfully created millionaire_fastlane\n",
      "Successfully created the-psychology-money\n",
      "Successfully created the-intelligent-investor\n",
      "Successfully created fast-and-slow\n",
      "Successfully created principles-life\n",
      "Successfully created teach-you-to-be-rich\n",
      "Successfully created the-little-book\n",
      "Successfully created the-millionaire\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "db1 = create_pgvector_db(\"rich_poor_dad\", \"rich_poor_dad\", processed_texts[0])\n",
    "db2 = create_pgvector_db(\"total-money\", \"total-money\", processed_texts[1])\n",
    "db3 = create_pgvector_db(\"your-money\", \"your-money\", processed_texts[2])\n",
    "db4 = create_pgvector_db(\"millionaire_fastlane\", \"millionaire_fastlane\", processed_texts[3])\n",
    "db5 = create_pgvector_db(\"the-psychology-money\", \"the-psychology-money\", processed_texts[4])\n",
    "db6 = create_pgvector_db(\"the-intelligent-investor\", \"the-intelligent-investor\", processed_texts[5])\n",
    "\n",
    "db1 = create_pgvector_db(\"fast-and-slow\", \"fast-and-slow\", processed_texts[6])\n",
    "db2 = create_pgvector_db(\"principles-life\", \"principles-life\", processed_texts[7])\n",
    "db3 = create_pgvector_db(\"teach-you-to-be-rich\", \"teach-you-to-be-rich\", processed_texts[8])\n",
    "db4 = create_pgvector_db(\"the-little-book\", \"the-little-book\", processed_texts[9])\n",
    "db5 = create_pgvector_db(\"the-millionaire\", \"tthe-millionaire\", processed_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
